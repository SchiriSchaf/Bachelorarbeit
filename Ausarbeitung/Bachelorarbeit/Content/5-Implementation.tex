%% ==============================
\chapter{\iflanguage{ngerman}{Implementierung}{Implementation}}
\label{sec:implementation}
%% ==============================




Nachdem im vorherigen Kapitel das Softwaredesign erstellt wurde, wird im Folgenden die Implementierung beschrieben.
\newline
Ruft der Benutzer ein Modul des Helpers über den jeweiligen Befehl in der Konsole auf, so wird die jeweilige \textit{Call} Methode, mit denen vom Benutzer gegebenen Parameter, ausgeführt.
Es existiert für jeden der Berechnungsschritte der Gradienten, LH-Werte, LH-Cluster und Räumlichen-Cluster eine eigene statische Klasse, die eine öffentliche Methode besitzt. Diese berechnet für die gegebenen Parameter den jeweiligen Schritt und gibt das Ergebnis zurück. 
Beispielsweise wird in der \textit{Call} Methode des \textit{LHHistogram} Moduls, zuerst die \textit{CalcGradientVolume} Funktion der \textit{Gradient} Klasse mit dem Intensitätsvolumen als Parameter aufgerufen. Danach wird die \textit{LHValuesVolume} Methode der \textit{LHValues} Klasse mit dem Ergebnis der vorherigen Funktion als Parameter ausgeführt. Aus dessen Ergebnis wird in der \textit{Call} Methode des Moduls direkt das LH-Histogramm erstellt und gespeichert.
Im Modul \textit{ClusterVolume} läuft die Berechnung ebenso über das Aufrufen von den Methoden der jeweiligen Klassen ab.
Das Modul \textit{MergeCluster} führt seine Berechnung komplett in der \textit{Call} Funktion aus, da die Kalkulation nicht sehr aufwändig ist.
Die Implementierung der Klassen und deren Methoden ist das Thema dieses Kapitels und wird im Folgenden genauer erläutert.



\subsection{Gradient}

Bei der  Kalkulation der Gradienten wird parallel über jeden Voxel im Intensitätsvolumen iteriert und die Implementierung von \cite{hong2003method} aufgerufen.
\newline
Für die Berechnung der Methode wird eine Gewichtung und die Koordinaten aller 64 Punkte im Koordinatensystem der lokalen Nachbarschaft benötigt. Da das gesamte Volumen die selbe Voxellänge hat und die Koordinaten in der lokalen Nachbarschaft immer gleich sind, können diese beiden Werte für alle 64 Nachbarn einmalig in einem Vorverarbeitungsschritt berechnet werden. Sie werden dabei in einem 64 Elemente großes Array, mit der gleichen Nummerierung gespeichert, wie in \autoref{fig:nachbarschaft} gezeigt. Bei der Kalkulation jedes Gradienten müssen lediglich die beiden Arrays durch iteriert werden, um die entsprechenden Gewichtung und Koordinaten jedes Nachbarn zu erhalten.

\subsection{LH-Werte}

In \autoref{lst:lh_code} ist der Code zur Berechnung der High-Werte zu sehen. Der Code liegt innerhalb einer while-Schleife, die so lange aufgerufen wird, bis die Berechnung des Low- und des High-Wertes abgeschlossen, also \textit{LisFinished} und \textit{HisFinished} true sind.
\newline
Die Berechnung des Low-Wertes ist ebenfalls in der Schleife und sieht bis auf die Richtung der neuen \textit{absVector} und \textit{secondAbsVector} gleich aus. Folglich sind alle hier gegebenen Erklärung und Anmerkungen ebenso auf die Berechnung der Low-Werte zu beziehen.


\begin{lstlisting}[caption = Code zur Berechnung der High-Werte, label = lst:lh_code]
if (!HisFinished)
{
		FVector3 absVectorH = volumeVectors[xH, yH, zH];
		FVector3 absVectorHNormalized = absVectorH.Normalize();
		int tempNewXH = (((int)Math.Ceiling(xH + (stepSize * absVectorHNormalized.X))));
		int tempNewYH = (((int)Math.Ceiling(yH + (stepSize * absVectorHNormalized.Y))));
		int tempNewZH = (((int)Math.Ceiling(zH + (stepSize * absVectorHNormalized.Z))));
		if (tempNewXH < volumeVectors.Dimension.X && tempNewYH < volumeVectors.Dimension.Y && tempNewZH < volumeVectors.Dimension.Z && !(tempNewXH < 0) && !(tempNewYH < 0) && !(tempNewZH < 0))
		{
				secondAbsVectorH = volumeVectors[tempNewXH, tempNewYH, tempNewZH].Normalize();
				highestIntensity.Add(gradientIntensity[xH, yH, zH]);
				if (absVectorH.Length() < extremumLength)
				{
						HisFinished = true;
				}else
				{
						if (highestIntensity[highestIntensity.Count - 1] == highestIntensity[highestIntensity.Count - 2] && highestIntensity.Count > 3){
								HisFinished = true;
						}
						xH = (((int)Math.Ceiling(xH + 0.5 * stepSize * (absVectorHNormalized.X + secondAbsVectorH.X))));
						yH = (((int)Math.Ceiling(yH + 0.5 * stepSize * (absVectorHNormalized.Y + secondAbsVectorH.Y))));
						zH = (((int)Math.Ceiling(zH + 0.5 * stepSize * (absVectorHNormalized.Z + secondAbsVectorH.Z))));
						if (xH > volumeVectors.Dimension.X-1 || yH > volumeVectors.Dimension.Y-1 || zH > volumeVectors.Dimension.Z-1|| xH < 0 || yH < 0 || zH < 0)
						{
								HisFinished = true;
						}
				}
		}else
		{
				HisFinished = true;
		}
}
\end{lstlisting}


Am Anfang eines Durchlaufes wird der normalisierte Gradienten des aktuellen Punktes in \textit{absVectorNormalized} gespeichert. Anschließend wird der Punkt des zweiten normalisierten Vektors in den Zeilen 5 bis 7 berechnet. Liegt dieser außerhalb des Volumens, so wird die Integration beendet. Liegt er jedoch innerhalb des Volumens, so wird der \textit{secondAbsVector} ausgelesen und der Intensitätswert des aktuellen Punktes zu der Liste \textit{highestIntensity} hinzugefügt. Ist der Gradient des aktuellen Punktes kleiner als die \textit{extremumLength}, welche im Falle von CT-Daten bei null liegt, wird die Integration beendet.
\newline
Andernfalls wird zunächst mithilfe der \textit{highestIntensity} Liste nach Schleifen gesucht. Bei sehr kleinen Gradienten, die jedoch größer als null sind, kann es vorkommen, dass das Verfahren immer wieder den selben Punkt findet und somit in einer Endlosschleife feststeckt.
Wie in Zeile 17s zu sehen ist, wird dabei jedoch erst getestet, ob die Liste schon mehr als 3 Einträge hat. Da anfangs vor der while-Schleife bereits der Intensitätswert des Startvoxels zur Liste hinzugefügt werden muss. Dies geschieht aus dem Grund, dass sonst, würde das Verfahren bei dem ersten Abbruchkriterium bei der ersten Iteration schon stoppen, die \textit{highestIntensity} Liste leer wäre, gäbe es kein Ergebnis für den High-Wert.
Das Testen von Schleifen könnte jedoch über die Koordinaten der iterierten Punkte geschehen, für den unwahrscheinlichem Fall, dass zwei durch die Integration hintereinander besuchte Punkte genau den selben Intensitätswert haben.
Anschließend wird der nächste Punkt der Integration mithilfe von \textit{absVectorNormalized} und \textit{secondAbsVecotr} gemäß Heun's Methode ermittelt, die im Kapitel \nameref{sec:methods} vorgestellt wurde. Erneut endet die Integration, falls der neu berechnete Punkt außerhalb des Volumens liegt.
\newline
Dieser Vorgang wiederholt sich für den High- als auch für den Low-Wert solange, bis die Integration aus einem der gegebenen Abbruchkriterien stoppt. In diesem Fall wird der letzte Eintrag der \textit{highestIntensity} Liste ausgelesen und als Ergebnis für den High-Wert gespeichert. Ebenso passiert dies mit der für die Low-Werte äquivalenten \textit{lowestIntensity} Liste.
Der Fall, dass ein Iterationsschritt in einer der gezeigten Formen außerhalb des Volumens liegt, und deshalb das Verfahren gestoppt wird, kommt in ungefähr 2\%-3\% der Fälle vor. Des Weiteren kommt es bei zirka 25\% aller Berechnung dazu, dass die LH-Werte vertauscht sind, also der Low- größer als der High-Wert ist. Dem wird entgegengewirkt, indem bei einem Vorkommen dieses Problems die beiden Werte vertauscht gespeichert werden. Es war noch nicht möglich, den Grund für diese Verwechslung herauszufinden.



\subsection{LH-Clustering}

Die Implementierung des LH-Clusterings wurde mit einer parallelen for-Schleife realisiert, die über die L-Werte mit der Schrittweite von 5 iteriert. Für jede Spalte i wird dann die in \autoref{alg:clustering} beschriebene Funktion aufgerufen.


\begin{algorithm}[h]
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

 \Input{LH-Histogramm, Spalte i}
 \Output{LH-Clusters}
 \BlankLine
 $AlleErgbnisCluster$\;
 $Mittelpunkt$\;
 $AlterMittelpunkt$\;
 $AktuellerCluster$\;
 \For{$j\leftarrow i$ \KwTo Rand des Histogramms, Schrittweite: 5}{
  $Mittelpunkt \leftarrow (i,j)$\;
 \While{Abstand(NeuerMittelpunkt, AlterMittelpunkt) > Threshold * Bandweite}{
  \For{$(k, l)\leftarrow$ Alle Punkte im Radius der Bandweite um den Mittelpunkt}{
      \If{NochNichtTeilDesClusters((k, l))}{
	$AktuellerCluster.Hinzufuegen((k, l))$\;
     }
    }
   $AlterMittelpunkt \leftarrow Mittelpunkt$\;
    $Mittelpunkt \leftarrow NeuenMittelpunktBerechnen(AktuellerCluster)$\;
  }
  $AlleErgebnisCluster.Hinzufügen(AktuellerCluster)$\;
 $ Leeren(AktuellerCluster)$\;
 }
$VerschmelzeNaheCluster(AlleErgebnisCluster)$\;
\Return{$AlleErgebnisCluster$}\;

 \caption{Pseudocode der Implementierung der LH-Cluster}
 \label{alg:clustering}
\end{algorithm}


Die erste for-Schleife iteriert über die H-Werte. Der Parameter j beginnt mit dem Wert i und wird wie dieser mit der Schrittweite 5 hochgezählt. Dies hat den Grund, dass es im LH-Histogramm keine Einträge gibt, bei denen der Low- höher als der High-Wert ist. Alle durch die beiden Schleifen entstehenden Punkte $(i, j)$ sind jeweils die Startpunkte einer Clustersuche.
Um nicht zu viele Cluster zwischenspeichern zu müssen und erst ganz am Ende alle ähnlichen Cluster zu verschmelzen, werden bereits nach der Berechnung jeder Spalte deren Ergebniscluster soweit wie möglich verschmolzen.
Weiterhin speichert der temporäre Cluster \textit{AktuellerCluster} lediglich die Koordinaten der zum Cluster dazugehörigen Punkte im Histogramm ab, jedoch nicht die räumlichen Informationen der darin gespeicherten Voxel. Erst am Ende der parallelen Schleife, wenn alle Ergebnisse gesammelt und die Cluster verschmolzen wurden, werden diese Daten ausgelesen. Dies spart Speicherplatz und Berechnungszeit, da in einem einzigen Kästchen im LH-Histogramm mehrere Tausend Voxel gespeichert sein können.


Das LH-Clustering wurde anfangs wie im Paper von Nguyen \cite{nguyen2012clustering} über das gesamte Histogramm mit einer Bandweite von 7\% des maximalen LH-Wertes für jeden Kasten berechnet. Dies ist aus mehreren Gründen suboptimal.
Zum einen ist der maximale LH-Wert sehr hoch, über 4000, obwohl nur zirka 0,3\% der Voxel einen Wert über 2400 haben. Zum anderen liegen über 50\% der LH-Werte unter 5. Die Kombination aus diesen Gründen machte das Clustering extrem langsam. Im Bereich von 0 bis 50  wurde in einem sehr großen Radius geclustert, wodurch mit jeder Iteration sehe viele Cluster gefunden und hinzugefügt werden mussten.
\newline
Eine erste Maßnahme um das Problem zu beheben, war es, alle Low- und High- Werten, die über 2400 waren, jeweils auf 2400 zu setzen. Dadurch wurden nur wenige Werte verändert und der Clusteringradius wurde deutlich kleiner. Dies konnte das beschriebene Problem jedoch nicht alleine lösen, da immer noch viele Punkte gefunden und dies für jeden Kasten im Histogramm durchgeführt werden mussten.
Also wurde als nächste Verbesserung eine Schrittweite beim Clustern eingeführt. Dies geschah aus der Beobachtung heraus, dass zwei bis drei oder eventuell sogar mehr direkt nebeneinanderliegende Kasten meist zum selben oder einen ähnlichen Cluster führten, und dass diese im letzten Schritt des Clusteringsverfahrens verschmolzen wurden.
Diese Änderung verbesserte die Berechnungszeit erneut, jedoch  dauerte die Berechnung immer noch relativ lange und es entstanden sehr viele Cluster. Diese durchzuschauen benötigte viel Zeit und das Gehirn wurde dabei meist als wenige, sehr große Cluster erkannt.
Daraufhin wurde das Clustering auf einen LH-Wertbereich von 1025 bis 1075 begrenzt und die Bandweite auf 0,1\% des maximalen LH-Wertes gesetzt. Diese Änderung führte zum Erfolg und das Ventrikelsystem war innerhalb der paar hundert Clustern mit ein wenig Aufwand zu erkennen.
\newline
Im LH-Histogramm eines CT-Datensatzes in \autoref{fig:lh_histo} sind viele dieser Beobachtungen zu erkennen. Es ist deutlich zu sehen, dass sehr wenige Werte über 2400 existieren. Des Weiteren ist die Ballungen an Kästchen mit sehr vielen Einträgen im linken unteren Eck mit LH-Werten zwischen  0 und 50 zu erkennen.
Weiterhin in dem kleinen Bereich, auf den der schwarze Pfeil zeigt, ebenfalls eine große Ansammlung an Kästchen mit sehr vielen Einträgen zu sehen. Diese Ansammlung an Punkten stellt das Gehirn dar und es ist zu erkennen, dass die Intensitäswerte des Gehirns sehr nah beieinander liegen.
Dies macht das genaue Clustern über diesen speziellen Bereich notwendig.

\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Logos/LHHistogram_a.png}
\caption{LH-Histogramm} 
\label{fig:lh_histo} 
\end{figure}



\subsection{Räumliches-Clustering}

Die Berechnung der räumlichen Cluster und des Volumens, in dem die Cluster IDs gespeichert sind, wird in der statischen Klasse \textit{SpatialClustering} ausgeführt.


\begin{lstlisting}[caption = Code zur Berechnung der räumlichen Cluster, label = lst:spatclust_code]
List<IVector3> currentSpatialCluster = new List<IVector3>();
		List<List<IVector3>> resultClusters = new List<List<IVector3>>();
		FVector3 midPoint;
		HashSet<IVector3> toRemove = new HashSet<IVector3>();
		while (LHClusters2[i].Count > ClusterSizeMin)
		{
				midPoint = LHClusters2[i].ElementAt(0);
				bool nothingMoreFound = false;
				while (!nothingMoreFound)
				{
						foreach( IVector3 current in LHClusters2[i])
						{
								if (InDistance(midPoint, current))
								{
										currentSpatialCluster.Add(current);
										toRemove.Add(current);
								}
						}
						foreach(IVector3 remove in toRemove){
								LHClusters2[i].Remove(remove);
						}
						toRemove.Clear();
						FVector3 newMidPoint = NewMidpoint(currentSpatialCluster);
						if (MinimumDistance > (SpatialDistance(newMidPoint, midPoint)))
						{
								nothingMoreFound = true;
						}
						else
						{
								midPoint = newMidPoint;
						}
				}
				if (currentSpatialCluster.Count > ClusterSizeMin)
				{
						resultClusters.Add(new List<IVector3>(currentSpatialCluster));		
				}
				currentSpatialCluster.Clear();
		}
		currentSpatialCluster.Clear();
		return resultClusters;

\end{lstlisting}


Die Kalkulation der räumlichen Clustern wurde dabei, wie die Berechnung der LH-Cluster, mit einer parallelen for-Schleife realisiert. Diese iteriert über die LH-Cluster und ruft für jeden Cluster die in \autoref{lst:spatclust_code} gezeigte Funktion auf. Die Cluster sind in der globalen Liste \textit{LHClusters} gespeichert. Sie haben den Typ einer Liste von \textit{IVector3}. 
Die Suche nach Clustern liegt in einer while-Schleife, die so lange ausgeführt wird, solange der LH-Cluster genug Elemente hat, damit ein Cluster mit der Mindestgröße an Elementen gefunden werden kann. 
Das Finden von Clustern läuft dabei folgendermaßen ab: Der erste Punkt im LH-Cluster wird als Startmittelpunkt gewählt. Danach wird durch alle Punkte iteriert und diejenigen, die innerhalb der Bandweite liegen, sowohl zum temporärem Cluster \textit{currentSpatialCluster} als auch zum HashSet der zu entfernenden Punkte \textit{toRemove} hinzugefügt.
Im Code ist dies in den Zeilen 7 bis 18 zu sehen.
\newline
Anschließend werden alle Elemente von \textit{toRemove} aus dem LH-Cluster entfernt und wie beim LH-Clustering auch ein neuer Mittelpunkt für den aktuellen Cluster errechnet.
Wenn das Abbruchkriterium der zwei naheliegenden aufeinanderfolgenden Mittelpunkte erfüllt ist, wird der temporäre Cluster \textit{currentSpatialCluster} zur Liste der Ergebniscluster \textit{resultClusters} hinzugefügt, falls er genug Elemente besitzt.
\newline
Für die Bandweite und die minimale Distanz des Abbruchkriteriums wurden im Paper von Nguyen \cite{nguyen2012clustering} keine Werte angegeben. In dieser Bachelorarbeit wurde die Bandweite auf 15 und die Distanz auf 0,01 gesetzt. Diese Werte wurden durch Testen herausgefunden und erwiesen sich als zielführend.
Anschließend wurde in der Funktion \textit{ComputeIDs}, der die Cluster als Parameter übergeben werden, das Clustervolumen erstellt und zurückgegeben.



\subsection{Unity}

Die Erweiterung, die am Renderer vorgenommen wurde, wurde im Shader programmiert. Die dazugehörigen hinzugefügten Parameterfelder sind in \autoref{fig:unity_para} zu sehen. Zuerst wurde eine drop-down-Liste \textit{Volume coloring method}  hinzugefügt, über die der Nutzer zwischen den Modi \textit{Default}, \textit{SpecificValue} und \textit{SpecificValueRange} wählen kann. Außerdem wurden drei Textfelder hinzugefügt.
\newline
Über das Parameterfeld \textit{Specific value} ist es dem Anwender möglich, den spezifischen Wert für den Modus \textit{SpecificValue} anzugeben. Weiterhin kann er über \textit{Minimum volume value} die untere Grenze und über \textit{Maximum volume value} die obere Grenze des Wertebereichs für den \textit{SpecificValueRange} Modus angeben. Die Grenzwerte sind dabei jeweils inklusive. Des Weiteren wurde noch das Farbfeld \textit{Color for specifiv value} hinzugefügt, welches die Farbe anzeigt, mit der hervorgehobenen Wert oder Wertebereiche dargestellt werden. Der Benutzer kann diese Farbe über das Parameterfeld verändern.
\newline
Der \textit{Default} Modus steht für die schon vorher dagewesene und im Kapitel \nameref{sec:concept} bereits beschriebene Implementierung der Farbgebung mit verschiedenen Grauwerten. Diese wurde in den beiden anderen beiden Modi übernommen. Der Unterschied besteht jedoch darin, dass allen Voxeln, die bei \textit{SpecificValue} den spezifischen Wert oder bei \textit{SpecificValueRange} innerhalb des Wertebereichs liegen, die Farbe des Farbfeldes \textit{Color for specifiv value} zugewiesen wird. Allen anderen Voxel erhalten Grauwerte wie im \textit{Default} Modus.


\begin{figure}[h]
\centering 
\includegraphics[width=\textwidth]{Logos/Unity_parameter.png}
\caption{Parameterfelder in Unity} 
\label{fig:unity_para} 
\end{figure}









































